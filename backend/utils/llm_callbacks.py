# -*- coding: utf-8 -*-
# Copyright (c) 2026 relakkes@gmail.com
#
# This file is part of MediaCrawlerPro-ContentRemixAgent project.
# Repository: https://github.com/MediaCrawlerPro/MediaCrawlerPro-ContentRemixAgent/blob/main/backend/utils/llm_callbacks.py
# GitHub: https://github.com/NanmiCoder
# Licensed under NON-COMMERCIAL LEARNING LICENSE 1.1
#
# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚

"""
LLM Debug Callback Handler - LLM è°ƒç”¨è°ƒè¯•å›è°ƒ

åŠŸèƒ½ï¼š
- è®°å½• LLM è¯·æ±‚å‚æ•°ï¼ˆmodel, temperature, promptç­‰ï¼‰
- è®°å½• LLM å“åº”å†…å®¹å’Œ token ä½¿ç”¨é‡
- è®°å½•è°ƒç”¨å»¶è¿Ÿ
- è®°å½•é”™è¯¯è¯¦æƒ…
- æ•æ„Ÿä¿¡æ¯è„±æ•ï¼ˆAPI Keyï¼‰
"""
import sys
import time
import json
import traceback
from typing import Any, Dict, List, Optional
from datetime import datetime

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from utils.logger import logger


class LLMDebugCallbackHandler(BaseCallbackHandler):
    """LLM è°ƒè¯•å›è°ƒå¤„ç†å™¨"""

    def __init__(
        self,
        log_level: str = "DEBUG",
        mask_api_keys: bool = True,
        max_content_length: int = 500,
    ):
        super().__init__()
        self.log_level = log_level
        self.mask_api_keys = mask_api_keys
        self.max_content_length = max_content_length
        self._call_times: Dict[str, float] = {}
        # ç¼“å­˜æœ€è¿‘çš„è¯·æ±‚ä¿¡æ¯ï¼Œç”¨äºé”™è¯¯æ—¶è¾“å‡º
        self._last_request: Dict[str, Any] = {}

    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        *,
        run_id: Any,
        parent_run_id: Optional[Any] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        """LLM è°ƒç”¨å¼€å§‹ - ç¼“å­˜è¯·æ±‚ä¿¡æ¯ï¼Œä»…åœ¨ DEBUG çº§åˆ«è®°å½•"""
        self._call_times[str(run_id)] = time.time()

        # æå–å…³é”®ä¿¡æ¯
        model_name = serialized.get("name", "unknown")
        model_kwargs = serialized.get("kwargs", {}).copy()

        # è„±æ•å¤„ç†
        if self.mask_api_keys and "api_key" in model_kwargs:
            api_key = model_kwargs["api_key"]
            model_kwargs["api_key"] = api_key[:12] + "..." if api_key else None

        # æˆªæ–­ prompt
        prompt_preview = prompts[0][:self.max_content_length] if prompts else ""
        full_prompt = prompts[0] if prompts else ""
        if prompts and len(prompts[0]) > self.max_content_length:
            prompt_preview += f"... ({len(prompts[0])} chars total)"

        # ç¼“å­˜è¯·æ±‚ä¿¡æ¯ï¼ˆç”¨äºé”™è¯¯æ—¶è¾“å‡ºï¼‰
        self._last_request[str(run_id)] = {
            "model": model_name,
            "temperature": model_kwargs.get("temperature"),
            "max_tokens": model_kwargs.get("max_tokens"),
            "base_url": model_kwargs.get("base_url"),
            "extra_body": model_kwargs.get("extra_body", {}),
            "reasoning": model_kwargs.get("reasoning", {}),
            "prompt_preview": prompt_preview,
            "prompt_full": full_prompt,
        }

        # æ­£å¸¸æƒ…å†µä¸‹ä½¿ç”¨ DEBUG çº§åˆ«ï¼ˆä¸å¹²æ‰°ï¼‰
        logger.debug(
            f"[LLM Request] run_id={run_id}\n"
            f"  Model: {model_name}\n"
            f"  Temperature: {model_kwargs.get('temperature')}\n"
            f"  Base URL: {model_kwargs.get('base_url')}\n"
            f"  Prompt: {prompt_preview}"
        )

    def on_llm_end(
        self,
        response: LLMResult,
        *,
        run_id: Any,
        parent_run_id: Optional[Any] = None,
        **kwargs: Any,
    ) -> None:
        """LLM è°ƒç”¨ç»“æŸ - ä»…åœ¨ DEBUG çº§åˆ«è®°å½•"""
        elapsed = time.time() - self._call_times.pop(str(run_id), time.time())

        # æ¸…ç†ç¼“å­˜çš„è¯·æ±‚ä¿¡æ¯
        self._last_request.pop(str(run_id), None)

        # æå– token ä½¿ç”¨é‡
        llm_output = response.llm_output or {}
        token_usage = llm_output.get("token_usage", {})

        # æå–å“åº”å†…å®¹
        generations = response.generations[0] if response.generations else []
        response_text = generations[0].text if generations else ""
        response_preview = response_text[:self.max_content_length]
        if len(response_text) > self.max_content_length:
            response_preview += f"... ({len(response_text)} chars total)"

        # æ­£å¸¸æƒ…å†µä¸‹ä½¿ç”¨ DEBUG çº§åˆ«ï¼ˆä¸å¹²æ‰°ï¼‰
        logger.debug(
            f"[LLM Response] run_id={run_id}\n"
            f"  Latency: {elapsed:.2f}s\n"
            f"  Tokens: {token_usage.get('total_tokens', 'N/A')}\n"
            f"  Response: {response_preview}"
        )

    def on_llm_error(
        self,
        error: Exception,
        *,
        run_id: Any,
        parent_run_id: Optional[Any] = None,
        **kwargs: Any,
    ) -> None:
        """LLM è°ƒç”¨é”™è¯¯ - è¾“å‡ºå®Œæ•´çš„è¯·æ±‚ä¸Šä¸‹æ–‡å’Œé”™è¯¯ä¿¡æ¯"""
        elapsed = time.time() - self._call_times.pop(str(run_id), time.time())

        # è·å–ç¼“å­˜çš„è¯·æ±‚ä¿¡æ¯
        request_info = self._last_request.pop(str(run_id), {})

        # åˆ†æé”™è¯¯ç±»å‹
        error_type = type(error).__name__
        error_msg = str(error)

        # ç‰¹æ®Šå¤„ç†å¸¸è§é”™è¯¯
        is_bad_request = "400" in error_msg
        is_rate_limit = "429" in error_msg or "rate limit" in error_msg.lower()
        is_timeout = "timeout" in error_msg.lower()
        is_reasoning_error = "Unknown parameter" in error_msg and "reasoning" in error_msg

        # æ„å»ºè¯¦ç»†çš„é”™è¯¯æ—¥å¿—ï¼ˆåŒ…å«è¯·æ±‚ä¸Šä¸‹æ–‡ï¼‰
        error_log = [
            "=" * 80,
            f"âŒ LLM API è°ƒç”¨å¤±è´¥ [run_id={run_id}]",
            "=" * 80,
            "",
            "ã€è¯·æ±‚ä¿¡æ¯ã€‘",
            f"  Model: {request_info.get('model', 'N/A')}",
            f"  Base URL: {request_info.get('base_url', 'N/A')}",
            f"  Temperature: {request_info.get('temperature', 'N/A')}",
            f"  Max Tokens: {request_info.get('max_tokens', 'N/A')}",
            f"  Extra Body: {json.dumps(request_info.get('extra_body', {}), ensure_ascii=False)}",
            f"  Reasoning: {json.dumps(request_info.get('reasoning', {}), ensure_ascii=False)}",
            "",
            "ã€Prompt (å‰500å­—ç¬¦)ã€‘",
            f"  {request_info.get('prompt_preview', 'N/A')}",
            "",
            "ã€é”™è¯¯è¯¦æƒ…ã€‘",
            f"  Error Type: {error_type}",
            f"  Error Message: {error_msg}",
            f"  Elapsed: {elapsed:.2f}s",
            "",
            "ã€é”™è¯¯åˆ†ç±»ã€‘",
            f"  âŒ 400 Bad Request: {is_bad_request}",
            f"  â±ï¸  429 Rate Limit: {is_rate_limit}",
            f"  ğŸ• Timeout: {is_timeout}",
            f"  ğŸ”§ Reasoning Parameter Error: {is_reasoning_error}",
            "",
            "ã€è§£å†³å»ºè®®ã€‘",
        ]

        # æ ¹æ®é”™è¯¯ç±»å‹æ·»åŠ å…·ä½“å»ºè®®
        if is_reasoning_error:
            error_log.extend([
                "  âš ï¸  æ£€æµ‹åˆ° reasoning å‚æ•°é”™è¯¯ï¼",
                "  ğŸ‘‰ è§£å†³æ–¹æ³•ï¼šåœ¨ .env ä¸­è®¾ç½® LLM_FORCE_DISABLE_REASONING=true",
                "  ğŸ‘‰ åŸå› ï¼šæ‚¨çš„ API endpoint ä¸æ”¯æŒ reasoning å‚æ•°",
            ])
        elif is_rate_limit:
            error_log.extend([
                "  âš ï¸  è§¦å‘äº† API é€Ÿç‡é™åˆ¶ï¼",
                "  ğŸ‘‰ è§£å†³æ–¹æ³•ï¼šé™ä½è¯·æ±‚é¢‘ç‡æˆ–å‡çº§ API plan",
            ])
        elif is_timeout:
            error_log.extend([
                "  âš ï¸  è¯·æ±‚è¶…æ—¶ï¼",
                "  ğŸ‘‰ è§£å†³æ–¹æ³•ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–å¢åŠ  timeout é…ç½®",
            ])
        elif is_bad_request:
            error_log.extend([
                "  âš ï¸  è¯·æ±‚å‚æ•°é”™è¯¯ï¼",
                "  ğŸ‘‰ è§£å†³æ–¹æ³•ï¼šæ£€æŸ¥ modelã€temperatureã€max_tokens ç­‰å‚æ•°æ˜¯å¦æ­£ç¡®",
            ])
        else:
            error_log.append("  ğŸ‘‰ æŸ¥çœ‹ä¸‹æ–¹å®Œæ•´é”™è¯¯å †æ ˆè·å–æ›´å¤šä¿¡æ¯")

        # ä» error å¯¹è±¡æå–å †æ ˆï¼ˆå› ä¸º on_llm_error å›è°ƒä¸­æ²¡æœ‰æ´»è·ƒçš„å¼‚å¸¸ä¸Šä¸‹æ–‡ï¼‰
        error_log.append("")
        error_log.append("ã€é”™è¯¯å †æ ˆã€‘")
        if error.__traceback__:
            # ä½¿ç”¨ format_exception ä»å¼‚å¸¸å¯¹è±¡æå–å †æ ˆ
            tb_lines = traceback.format_exception(type(error), error, error.__traceback__)
            error_log.append("".join(tb_lines))
        else:
            error_log.append(f"  {error_type}: {error_msg}")
        error_log.append("=" * 80)
        error_log.append("")

        # ç›´æ¥æ‰“å°é”™è¯¯ä¿¡æ¯åˆ° stderrï¼ˆé¿å… loguru æ ¼å¼åŒ–é—®é¢˜ï¼‰
        print("\n".join(error_log), file=sys.stderr)

        # ä½¿ç”¨ logger.error è®°å½•ï¼ˆæ³¨æ„ï¼šon_llm_error å›è°ƒä¸­æ²¡æœ‰æ´»è·ƒå¼‚å¸¸ä¸Šä¸‹æ–‡ï¼Œä¸èƒ½ç”¨ exceptionï¼‰
        logger.error(f"LLM API è°ƒç”¨å¤±è´¥: {error_type}: {error_msg}")


def get_debug_callback_handler() -> LLMDebugCallbackHandler:
    """æ¯æ¬¡è°ƒç”¨è¿”å›æ–°çš„ callback handler å®ä¾‹

    é¿å…è·¨è¯·æ±‚çš„çŠ¶æ€æ³„æ¼ã€‚æ¯ä¸ªè¯·æ±‚æœ‰ç‹¬ç«‹çš„ _call_times å’Œ _last_request å­—å…¸ã€‚
    """
    return LLMDebugCallbackHandler(
        log_level="DEBUG",
        mask_api_keys=True,
        max_content_length=500,
    )
